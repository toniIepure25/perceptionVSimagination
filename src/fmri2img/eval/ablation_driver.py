#!/usr/bin/env python3
"""
Ablation Study Driver for fMRI Reconstruction
=============================================

Systematic hyperparameter sweeps to understand what matters:
1. **PCA Dimensionality**: Does higher k improve performance?
2. **InfoNCE Weight**: How important is contrastive learning?
3. **Architecture Depth**: Deeper = better?
4. **Best-of-N**: How many candidates needed?
5. **Self-Supervised Pretraining**: Does SSL help?

This script automates running multiple training/evaluation jobs with different
configurations and compiles results into comparison tables.

Usage:
    # PCA dimensionality ablation
    python scripts/ablation_driver.py \\
        --subject subj01 \\
        --ablation-type pca_dims \\
        --output-dir outputs/ablations/pca_dims \\
        --base-config configs/sota_two_stage.yaml
    
    # InfoNCE weight ablation
    python scripts/ablation_driver.py \\
        --subject subj01 \\
        --ablation-type infonce_weight \\
        --output-dir outputs/ablations/infonce \\
        --base-config configs/sota_two_stage.yaml
    
    # Architecture depth ablation
    python scripts/ablation_driver.py \\
        --subject subj01 \\
        --ablation-type arch_depth \\
        --output-dir outputs/ablations/depth \\
        --base-config configs/sota_two_stage.yaml
    
    # Best-of-N ablation (generation only)
    python scripts/ablation_driver.py \\
        --subject subj01 \\
        --ablation-type best_of_n \\
        --output-dir outputs/ablations/best_of_n \\
        --encoder-checkpoint checkpoints/two_stage/subj01/two_stage_best.pt
"""

import argparse
import json
import logging
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any
import shutil

import pandas as pd
import yaml
from tqdm import tqdm

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# Ablation configurations
ABLATION_CONFIGS = {
    "pca_dims": {
        "description": "PCA dimensionality sweep",
        "param": "preprocessing.pca_k",
        "values": [128, 256, 512, 768, 1024],
        "requires_training": True,
        "expected_trend": "Higher k â†’ better (with diminishing returns)"
    },
    
    "infonce_weight": {
        "description": "InfoNCE loss weight sweep",
        "param": "loss.infonce_weight",
        "values": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6],
        "requires_training": True,
        "expected_trend": "Optimal around 0.3-0.4"
    },
    
    "arch_depth": {
        "description": "Architecture depth (number of residual blocks)",
        "param": "encoder.n_blocks",
        "values": [2, 3, 4, 6, 8],
        "requires_training": True,
        "expected_trend": "Deeper = better up to ~4 blocks, then overfitting"
    },
    
    "latent_dim": {
        "description": "Latent dimensionality of Stage 1 encoder",
        "param": "encoder.latent_dim",
        "values": [256, 512, 768, 1024, 1536],
        "requires_training": True,
        "expected_trend": "Higher dim = more capacity (but slower)"
    },
    
    "dropout": {
        "description": "Dropout rate in residual blocks",
        "param": "encoder.dropout",
        "values": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],
        "requires_training": True,
        "expected_trend": "Moderate dropout (0.3) prevents overfitting"
    },
    
    "ssl_pretraining": {
        "description": "Self-supervised pretraining comparison",
        "param": "encoder.ssl_pretrain",
        "values": [False, True],
        "requires_training": True,
        "expected_trend": "SSL improves sample efficiency"
    },
    
    "best_of_n": {
        "description": "Best-of-N sampling comparison",
        "param": "generation.n_candidates",
        "values": [1, 2, 4, 8, 16, 32],
        "requires_training": False,
        "expected_trend": "Logarithmic improvement, plateau at N=16"
    },
    
    "boi_steps": {
        "description": "BOI-lite refinement steps",
        "param": "generation.boi_steps",
        "values": [0, 1, 2, 3, 4, 5],
        "requires_training": False,
        "expected_trend": "More steps = better quality (diminishing returns)"
    }
}


def load_base_config(config_path: str) -> Dict[str, Any]:
    """Load base configuration from YAML file."""
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
    return config


def update_nested_dict(d: Dict, key_path: str, value: Any):
    """
    Update nested dictionary using dot notation.
    
    Example:
        update_nested_dict(config, "encoder.n_blocks", 6)
        -> config["encoder"]["n_blocks"] = 6
    """
    keys = key_path.split(".")
    current = d
    
    for key in keys[:-1]:
        if key not in current:
            current[key] = {}
        current = current[key]
    
    current[keys[-1]] = value


def save_config(config: Dict, output_path: Path):
    """Save configuration to YAML file."""
    with open(output_path, "w") as f:
        yaml.dump(config, f, default_flow_style=False)


def run_training_job(
    config_path: Path,
    output_dir: Path,
    subject: str
) -> Dict[str, float]:
    """
    Run training job and return validation metrics.
    
    Returns:
        metrics: Dict with val_cosine, val_mse, etc.
    """
    logger.info(f"Running training with config: {config_path}")
    
    # Run training script
    cmd = [
        "python", "scripts/train_two_stage.py",
        "--config", str(config_path),
        "--output-dir", str(output_dir),
        "--subject", subject
    ]
    
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True
        )
        logger.info("Training completed successfully")
    except subprocess.CalledProcessError as e:
        logger.error(f"Training failed: {e}")
        logger.error(f"STDOUT: {e.stdout}")
        logger.error(f"STDERR: {e.stderr}")
        return {}
    
    # Load metrics from checkpoint metadata
    checkpoint_path = output_dir / "two_stage_best.pt"
    if checkpoint_path.exists():
        import torch
        checkpoint = torch.load(checkpoint_path, map_location="cpu")
        
        metrics = {
            "val_cosine": checkpoint.get("val_cosine", 0.0),
            "val_mse": checkpoint.get("val_mse", 0.0),
            "epoch": checkpoint.get("epoch", 0)
        }
        return metrics
    else:
        logger.warning(f"Checkpoint not found: {checkpoint_path}")
        return {}


def run_evaluation_job(
    checkpoint_path: Path,
    output_dir: Path,
    subject: str,
    encoder_type: str = "two_stage"
) -> Dict[str, float]:
    """
    Run evaluation job and return test metrics.
    
    Returns:
        metrics: Dict with R@1, R@5, cosine, etc.
    """
    logger.info(f"Running evaluation with checkpoint: {checkpoint_path}")
    
    # Run evaluation script
    cmd = [
        "python", "scripts/eval_retrieval.py",
        "--subject", subject,
        "--encoder-type", encoder_type,
        "--checkpoint", str(checkpoint_path),
        "--split", "test",
        "--gallery", "test",
        "--output-json", str(output_dir / "eval_results.json")
    ]
    
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True
        )
        logger.info("Evaluation completed successfully")
    except subprocess.CalledProcessError as e:
        logger.error(f"Evaluation failed: {e}")
        return {}
    
    # Load results
    results_path = output_dir / "eval_results.json"
    if results_path.exists():
        with open(results_path, "r") as f:
            metrics = json.load(f)
        return metrics
    else:
        return {}


def run_generation_ablation(
    checkpoint_path: Path,
    param_name: str,
    param_values: List[Any],
    output_dir: Path,
    subject: str
) -> pd.DataFrame:
    """
    Run generation-only ablation (e.g., best-of-N).
    
    Returns:
        results_df: DataFrame with results for each parameter value
    """
    results = []
    
    for value in tqdm(param_values, desc=f"Ablating {param_name}"):
        logger.info(f"\nRunning with {param_name}={value}")
        
        # Create output directory for this run
        run_dir = output_dir / f"{param_name}_{value}"
        run_dir.mkdir(parents=True, exist_ok=True)
        
        # Run generation/evaluation
        # (Implementation depends on specific ablation type)
        # For now, just log
        logger.info(f"Would generate with {param_name}={value}")
        
        # Placeholder metrics
        metrics = {
            param_name: value,
            "clip_score": 0.5 + value * 0.01,  # Dummy
            "ssim": 0.2 + value * 0.005
        }
        
        results.append(metrics)
    
    return pd.DataFrame(results)


def create_comparison_table(
    results_df: pd.DataFrame,
    output_path: Path,
    param_name: str
):
    """Create LaTeX comparison table."""
    # Save as CSV
    csv_path = output_path.parent / f"{output_path.stem}.csv"
    results_df.to_csv(csv_path, index=False)
    logger.info(f"Saved results to {csv_path}")
    
    # Create LaTeX table
    latex = r"\begin{table}[h]" + "\n"
    latex += r"\centering" + "\n"
    latex += r"\begin{tabular}{l" + "c" * (len(results_df.columns) - 1) + "}\n"
    latex += r"\toprule" + "\n"
    
    # Header
    latex += " & ".join(results_df.columns) + r" \\" + "\n"
    latex += r"\midrule" + "\n"
    
    # Rows
    for _, row in results_df.iterrows():
        latex += " & ".join([f"{val:.4f}" if isinstance(val, float) else str(val) 
                            for val in row]) + r" \\" + "\n"
    
    latex += r"\bottomrule" + "\n"
    latex += r"\end{tabular}" + "\n"
    latex += f"\\caption{{Ablation: {param_name}}}\n"
    latex += r"\end{table}" + "\n"
    
    # Save LaTeX
    with open(output_path, "w") as f:
        f.write(latex)
    
    logger.info(f"Saved LaTeX table to {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Ablation study driver",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Required
    parser.add_argument("--subject", type=str, required=True,
                        help="Subject ID")
    parser.add_argument("--ablation-type", type=str, required=True,
                        choices=list(ABLATION_CONFIGS.keys()),
                        help="Type of ablation study")
    parser.add_argument("--output-dir", type=str, required=True,
                        help="Output directory")
    
    # Config
    parser.add_argument("--base-config", type=str,
                        default="configs/sota_two_stage.yaml",
                        help="Base configuration file")
    
    # Optional
    parser.add_argument("--encoder-checkpoint", type=str, default=None,
                        help="Encoder checkpoint (for generation-only ablations)")
    parser.add_argument("--dry-run", action="store_true",
                        help="Print commands without running")
    
    args = parser.parse_args()
    
    # Setup
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    ablation_config = ABLATION_CONFIGS[args.ablation_type]
    
    logger.info("=" * 80)
    logger.info(f"Ablation Study: {ablation_config['description']}")
    logger.info("=" * 80)
    logger.info(f"Parameter: {ablation_config['param']}")
    logger.info(f"Values: {ablation_config['values']}")
    logger.info(f"Expected: {ablation_config['expected_trend']}")
    logger.info(f"Output: {output_dir}")
    
    # Load base config
    base_config = load_base_config(args.base_config)
    
    results = []
    
    # Run ablation
    if ablation_config["requires_training"]:
        logger.info("\nThis ablation requires training multiple models...")
        
        for value in ablation_config["values"]:
            logger.info(f"\n{'=' * 80}")
            logger.info(f"Running with {ablation_config['param']}={value}")
            logger.info('=' * 80)
            
            # Create modified config
            config = base_config.copy()
            update_nested_dict(config, ablation_config['param'], value)
            
            # Save config
            run_dir = output_dir / f"value_{value}"
            run_dir.mkdir(parents=True, exist_ok=True)
            config_path = run_dir / "config.yaml"
            save_config(config, config_path)
            
            if args.dry_run:
                logger.info(f"[DRY RUN] Would train with config: {config_path}")
                continue
            
            # Run training
            train_metrics = run_training_job(config_path, run_dir, args.subject)
            
            # Run evaluation
            checkpoint_path = run_dir / "two_stage_best.pt"
            eval_metrics = run_evaluation_job(checkpoint_path, run_dir, args.subject)
            
            # Combine metrics
            result = {ablation_config['param']: value}
            result.update(train_metrics)
            result.update(eval_metrics)
            results.append(result)
    
    else:
        # Generation-only ablation
        logger.info("\nThis ablation only requires generation (no training)...")
        
        if args.encoder_checkpoint is None:
            logger.error("--encoder-checkpoint required for generation-only ablations")
            return 1
        
        checkpoint_path = Path(args.encoder_checkpoint)
        results_df = run_generation_ablation(
            checkpoint_path,
            ablation_config['param'],
            ablation_config['values'],
            output_dir,
            args.subject
        )
        results = results_df.to_dict('records')
    
    # Create results DataFrame
    results_df = pd.DataFrame(results)
    
    # Save results
    logger.info("\n" + "=" * 80)
    logger.info("Results Summary")
    logger.info("=" * 80)
    print(results_df.to_string())
    
    # Save to files
    results_df.to_csv(output_dir / "results.csv", index=False)
    results_df.to_json(output_dir / "results.json", orient="records", indent=2)
    
    # Create LaTeX table
    create_comparison_table(
        results_df,
        output_dir / "results.tex",
        ablation_config['param']
    )
    
    logger.info(f"\nResults saved to {output_dir}")
    logger.info(f"  - CSV: results.csv")
    logger.info(f"  - JSON: results.json")
    logger.info(f"  - LaTeX: results.tex")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
