# =============================================================================
# DEPRECATED - Please use two_stage_sota.yaml instead
# =============================================================================
#
# This file is kept for backward compatibility.
# New experiments should use: configs/two_stage_sota.yaml
#
# Redirect configuration
# =============================================================================

_base_: two_stage_sota.yaml

# Note: This configuration is functionally equivalent to two_stage_sota.yaml
# but we recommend migrating to the new naming convention for clarity.

dataset:
  subject: subj01
  subject_num: 1
  max_trials: 30000
  train_ratio: 0.80 # 24,000 train
  val_ratio: 0.10 # 3,000 val
  test_ratio: 0.10 # 3,000 test
  index_dir: data/indices/nsd_index

preprocessing:
  reliability_threshold: 0.1
  pca_k: 512 # Higher than baseline (100) for more signal retention
  use_roi: false

# Two-Stage Encoder Configuration
encoder:
  type: "two_stage" # Options: "mlp", "two_stage"

  # Stage 1: fMRI → latent representation
  latent_dim: 768 # Latent brain representation dimensionality
  n_blocks: 4 # Number of residual blocks (3-6)
  dropout: 0.3 # Dropout for regularization

  # Stage 2: latent → CLIP embedding
  head_type: "mlp" # Options: "linear", "mlp" (ignored if shared_head_backbone=true)
  head_hidden_dim: 512 # Hidden dimension for MLP head or shared backbone

  # Phase 2 Enhancement: Shared head backbone for parameter efficiency
  shared_head_backbone: true # If true, use shared backbone + lightweight projections
  # Recommended: true for multi-layer mode (reduces params by ~60%)

  # Self-supervised pretraining (optional)
  self_supervised: false # Enable/disable pretraining
  ssl_objective: "masked" # Options: "masked", "denoising"
  ssl_epochs: 20 # Pretraining epochs
  mask_ratio: 0.3 # For masked autoencoder
  noise_std: 0.1 # For denoising autoencoder

  # Staged training (optional)
  freeze_stage1: false # Freeze Stage 1 after pretraining
  stage2_epochs: 30 # Epochs for Stage 2 if freezing Stage 1

# Loss function configuration
loss:
  mse_weight: 0.3 # Weight for MSE loss
  cosine_weight: 0.3 # Weight for cosine similarity loss
  info_nce_weight: 0.4 # Weight for InfoNCE contrastive loss
  temperature: 0.05 # Temperature for InfoNCE (0.01-0.1)

  # Phase 3 Enhancement: Multi-layer InfoNCE
  use_multilayer_infonce: true # If true, use combined multi-layer representation for InfoNCE
  infonce_combination: "weighted_pool" # Options: "weighted_pool", "concat_project", "average"
  # Recommended: "weighted_pool" (balances all layers with minimal params)

  # Brain-consistency (cycle) loss (Phase 2 - optional)
  brain_consistency_weight: 0.1 # Weight for cycle loss (0.0 = disabled, try 0.05-0.2)
  clip_to_fmri_encoder: null # Path to CLIP→fMRI encoder checkpoint (required if weight > 0)
  # Example: "checkpoints/clip_to_fmri/subj01/encoder.pt"

# Multi-layer CLIP supervision (Phase 3)
multi_layer:
  enabled: true # Enable multi-layer supervision from ViT intermediate layers
  cache_path: "cache/clip_embeddings/nsd_clipcache_multilayer.parquet"

  # Layer weights for supervision (should sum to ~1.0)
  # Phase 1 Enhancement: Can use fixed or learnable weights
  use_learnable_weights: true # If true, learn optimal weights during training
  layer_weights:
    layer_4: 0.15 # Early visual features (edges, textures)
    layer_8: 0.20 # Mid-level features (parts, patterns)
    layer_12: 0.25 # Late semantic features (objects, concepts)
    final: 0.40 # Final CLIP embedding (global representation)

  # Loss configuration
  use_mse: false # Add MSE component to cosine loss
  mse_weight: 0.1 # Weight for MSE if enabled

  # Scientific Rationale:
  # - Multi-level supervision improves gradient flow (Lin et al. 2017)
  # - Different layers capture different semantic levels (Raghu et al. 2021)
  # - Expected +5-10% embedding similarity improvement (Li et al. 2023)

# Multi-task semantics (Phase 2 - text-CLIP)
multi_task:
  predict_text_clip: false # Enable text-CLIP prediction alongside image-CLIP
  text_clip_cache: "cache/clip_embeddings/text_clip.parquet" # BLIP-2 captions + CLIP text embeddings
  text_clip_weight: 0.3 # Weight for text-CLIP loss (0.0 = disabled, try 0.2-0.5)
  # If enabled: loss = (1-w)*image_loss + w*text_loss
  # Scientific Rationale:
  # - Multi-task learning improves semantic alignment (Ruder 2017)
  # - Text supervision adds linguistic grounding (Radford et al. 2021)
  # - Expected improvement in semantic retrieval and caption alignment

# Training configuration
training:
  learning_rate: 0.001
  weight_decay: 0.0001
  batch_size: 128 # Increased from baseline (64) for better InfoNCE
  epochs: 50
  early_stop_patience: 10
  device: "cuda"
  seed: 42
  num_workers: 4

# CLIP Adapter (optional second stage)
adapter:
  enabled: false # Set to true to train adapter after encoder
  hidden_dim: 1536
  dropout: 0.0
  learning_rate: 0.0003
  batch_size: 256
  epochs: 50
  use_layernorm: true

# Diffusion configuration (for inference)
diffusion:
  model_id: "stabilityai/stable-diffusion-2-1"
  num_inference_steps: 250
  guidance_scale: 7.5
  scheduler: "dpm"
  eta: 0.0
  output_size: 768
  dtype: "float32"

  # Best-of-N sampling (to be implemented)
  best_of_n: 1 # Set >1 to enable (e.g., 8, 16)

  # BOI-lite refinement (to be implemented)
  boi_lite:
    enabled: false
    steps: 3
    candidates_per_step: 4

paths:
  output_dir: "outputs"
  cache_dir: "cache"
  checkpoint_dir: "checkpoints/two_stage"
  log_dir: "logs/two_stage"

# Ablation study configurations (optional)
ablations:
  # PCA dimensionality sweep
  pca_dims: [256, 512, 768]

  # InfoNCE ablation
  test_without_infonce: false

  # Architecture ablation
  n_blocks_sweep: [2, 3, 4, 6]
  latent_dim_sweep: [512, 768, 1024]

# Metadata
config_version: "1.0-sota"
description: "SOTA two-stage encoder with InfoNCE and residual architecture"
created: "2025-11-15"
baseline_comparison: "configs/production_improved.yaml"
