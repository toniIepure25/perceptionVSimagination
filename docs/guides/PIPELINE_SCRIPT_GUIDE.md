# ğŸš€ Full Pipeline Script Usage Guide

## Quick Start

The `run_full_pipeline.py` script is a **production-grade orchestrator** that handles the complete end-to-end workflow with intelligent caching and validation.

## âš¡ Common Commands

### 1. Full Novel Pipeline (Recommended)
```bash
python scripts/run_full_pipeline.py --subject subj01 --mode novel
```

**What it does**:
- âœ… Validates environment (Python, CUDA, disk space)
- âœ… Builds NSD index (if not cached)
- âœ… Builds CLIP cache (if not cached)
- âœ… Runs preprocessing with **soft reliability weighting** â­
- âœ… Trains MLP with **InfoNCE loss** â­
- âœ… Evaluates retrieval + similarity
- âœ… Evaluates **MC dropout uncertainty** â­
- âœ… Generates reports

**Time**: 
- First run: ~5-6 hours (includes CLIP cache building)
- Subsequent runs: ~2-3 hours (cache reused)

---

### 2. Full Ablation Study (All 4 Experiments)
```bash
python scripts/run_full_pipeline.py --subject subj01 --mode ablation
```

**What it does**: Runs 4 experiments sequentially:
1. **Baseline** - Hard threshold + no InfoNCE
2. **Soft Only** - Soft weights + no InfoNCE
3. **InfoNCE Only** - Hard threshold + InfoNCE
4. **Full Novel** - Soft weights + InfoNCE

**Output**: Comparison table showing relative improvements

**Time**: ~8-10 hours total

---

### 3. Baseline Only (For Comparison)
```bash
python scripts/run_full_pipeline.py --subject subj01 --mode baseline
```

**What it does**: Runs traditional approach (hard threshold, no InfoNCE)

---

## ğŸ”§ Advanced Options

### Resume from Specific Step
```bash
# Resume from training (skip index/cache/preproc)
python scripts/run_full_pipeline.py --subject subj01 --mode novel --resume-from train

# Resume from evaluation (skip everything except eval)
python scripts/run_full_pipeline.py --subject subj01 --mode novel --resume-from eval
```

**Available resume points**:
- `index` - Build index only
- `clip` - Build CLIP cache (assumes index exists)
- `preproc` - Run preprocessing (assumes index + cache exist)
- `train` - Run training (assumes preproc exists)
- `eval` - Run evaluation (assumes training complete)
- `uncertainty` - Run uncertainty eval only

---

### Force Rebuild (Ignore Cache)
```bash
python scripts/run_full_pipeline.py --subject subj01 --mode novel --force-rebuild
```

**When to use**: 
- Changed hyperparameters
- Suspect corrupted cache
- Want fresh computation

**Warning**: Will rebuild CLIP cache (~2-3 hours)

---

### Dry Run (Preview Commands)
```bash
python scripts/run_full_pipeline.py --subject subj01 --mode novel --dry-run
```

**What it does**: Shows all commands that **would** be executed without actually running them

**Use case**: Check what will happen before committing

---

### Skip Evaluation (Training Only)
```bash
python scripts/run_full_pipeline.py --subject subj01 --mode novel --skip-eval
```

**Use case**: Testing training configs without waiting for evaluation

---

## ğŸ“Š What Gets Cached?

The script **intelligently caches** every step to avoid redundant computation:

| Step | Cache Location | Validation |
|------|---------------|------------|
| **Index** | `data/indices/nsd_index/` | Checks row count + required columns |
| **CLIP Cache** | `outputs/clip_cache/clip.parquet` | Checks 73K images + embedding dim=512 |
| **Preprocessing** | `outputs/preproc/{config}/` | Checks all .npy files + soft weights |
| **Training** | `checkpoints/mlp/{config}/` | Checks best_model.pt exists |
| **Evaluation** | `outputs/eval/{config}/` | Checks metrics.json exists |
| **Uncertainty** | `outputs/eval/{config}_uncertainty/` | Checks uncertainty_summary.json |

**Smart Resume**: If a cached artifact fails validation, it's automatically rebuilt!

---

## ğŸ“ Output Structure

After running the pipeline, you'll have:

```
perceptionVSimagination/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ indices/
â”‚       â””â”€â”€ nsd_index/
â”‚           â””â”€â”€ subject=subj01/
â”‚               â””â”€â”€ index.parquet  âœ… Trial index
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ clip_cache/
â”‚   â”‚   â””â”€â”€ clip.parquet  âœ… 73K CLIP embeddings
â”‚   â”‚
â”‚   â”œâ”€â”€ preproc/
â”‚   â”‚   â”œâ”€â”€ baseline/
â”‚   â”‚   â”‚   â””â”€â”€ subj01/  âœ… Hard threshold artifacts
â”‚   â”‚   â”œâ”€â”€ soft_only/
â”‚   â”‚   â”‚   â””â”€â”€ subj01/  âœ… Soft weights artifacts
â”‚   â”‚   â”œâ”€â”€ infonce_only/
â”‚   â”‚   â”‚   â””â”€â”€ subj01/
â”‚   â”‚   â””â”€â”€ full_novel_both/
â”‚   â”‚       â””â”€â”€ subj01/  âœ… Soft + InfoNCE artifacts
â”‚   â”‚
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â”œâ”€â”€ baseline/
â”‚   â”‚   â”‚   â””â”€â”€ metrics.json  âœ… Standard metrics
â”‚   â”‚   â”œâ”€â”€ full_novel_both/
â”‚   â”‚   â”‚   â””â”€â”€ metrics.json
â”‚   â”‚   â”œâ”€â”€ baseline_uncertainty/
â”‚   â”‚   â”‚   â”œâ”€â”€ uncertainty_summary.json  âœ… Uncertainty metrics
â”‚   â”‚   â”‚   â”œâ”€â”€ uncertainty_results.csv
â”‚   â”‚   â”‚   â””â”€â”€ calibration_curve.png  âœ… Calibration plot
â”‚   â”‚   â””â”€â”€ full_novel_both_uncertainty/
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ reports/
â”‚       â””â”€â”€ comparison_subj01_ablation.csv  âœ… Comparison table
â”‚
â””â”€â”€ checkpoints/
    â””â”€â”€ mlp/
        â”œâ”€â”€ baseline/
        â”‚   â””â”€â”€ subj01/
        â”‚       â””â”€â”€ best_model.pt  âœ… Trained model
        â”œâ”€â”€ soft_only/
        â”œâ”€â”€ infonce_only/
        â””â”€â”€ full_novel_both/
            â””â”€â”€ subj01/
                â”œâ”€â”€ best_model.pt
                â”œâ”€â”€ final_model.pt
                â””â”€â”€ training_log.json  âœ… Loss components per epoch
```

---

## ğŸ¯ Real-World Examples

### Example 1: First-Time User
```bash
# Run full novel pipeline for first time
python scripts/run_full_pipeline.py --subject subj01 --mode novel

# Expected output:
# âœ“ Environment validated
# âœ“ Building NSD index (10 min)
# âœ“ Building CLIP cache (2-3 hours) â±ï¸
# âœ“ Preprocessing with soft weights (10 min)
# âœ“ Training with InfoNCE (2 hours) â±ï¸
# âœ“ Standard evaluation (10 min)
# âœ“ Uncertainty evaluation (15 min)
# âœ“ Pipeline complete: 5h 45m
```

### Example 2: Rerun After Cache Built
```bash
# Second run (cache exists)
python scripts/run_full_pipeline.py --subject subj01 --mode novel

# Expected output:
# âœ“ Environment validated
# âœ“ Using cached index [CACHED]
# âœ“ Using cached CLIP embeddings [CACHED]
# âœ“ Using cached preprocessing [CACHED]
# âœ“ Using cached checkpoint [CACHED]
# âœ“ Using cached evaluation [CACHED]
# âœ“ Using cached uncertainty [CACHED]
# âœ“ Pipeline complete: 2m (all cached!)
```

### Example 3: Changed Training Config
```bash
# Retrain with different InfoNCE weight
# (manually edit configs in script or use --force-rebuild)

python scripts/run_full_pipeline.py --subject subj01 --mode novel --resume-from train

# Expected output:
# âœ“ Using cached index [CACHED]
# âœ“ Using cached CLIP cache [CACHED]
# âœ“ Using cached preprocessing [CACHED]
# âœ“ Training with InfoNCE (2 hours) â±ï¸
# âœ“ Standard evaluation (10 min)
# âœ“ Uncertainty evaluation (15 min)
# âœ“ Pipeline complete: 2h 25m
```

### Example 4: Full Ablation Study
```bash
# Run all 4 experiments for paper
python scripts/run_full_pipeline.py --subject subj01 --mode ablation

# Expected output:
# [Experiment 1: Baseline]
# âœ“ Preprocessing (hard threshold)
# âœ“ Training (no InfoNCE)
# âœ“ Evaluation
# 
# [Experiment 2: Soft Only]
# âœ“ Preprocessing (soft weights) â­
# âœ“ Training (no InfoNCE)
# âœ“ Evaluation
# 
# [Experiment 3: InfoNCE Only]
# âœ“ Preprocessing (hard threshold)
# âœ“ Training (InfoNCE) â­
# âœ“ Evaluation
# 
# [Experiment 4: Full Novel]
# âœ“ Preprocessing (soft weights) â­
# âœ“ Training (InfoNCE) â­
# âœ“ Evaluation
# 
# âœ“ Comparison report generated
# 
# Comparison Summary:
# 
# config               cosine_similarity  retrieval_top1  retrieval_top5
# Baseline             0.8123             0.2345          0.4521
# Soft Only            0.8234             0.2456          0.4632
# InfoNCE Only         0.8187             0.3012          0.5234  <-- Big retrieval boost
# Full Novel (Both)    0.8312             0.3156          0.5498  <-- Best overall
# 
# âœ“ Pipeline complete: 9h 12m
```

---

## ğŸ› Troubleshooting

### "FileNotFoundError: index.parquet not found"
**Cause**: Index not built  
**Fix**: Run with `--resume-from index` or remove `.pipeline_state_*.json`

### "CLIP cache validation failed: only 1234 images"
**Cause**: Interrupted CLIP cache build  
**Fix**: Run with `--force-rebuild` to restart from scratch

### "No reliability_weights.npy found"
**Cause**: Preprocessing ran with `hard_threshold` mode  
**Fix**: This is expected for baseline experiments. Only `soft_weight` mode creates this file.

### "Training failed: CUDA out of memory"
**Cause**: Batch size too large  
**Fix**: Edit `configs` in script to reduce `batch_size` (try 32 or 16)

### Pipeline state corrupted
**Fix**: Delete state file and restart
```bash
rm .pipeline_state_subj01_novel.json
python scripts/run_full_pipeline.py --subject subj01 --mode novel
```

---

## ğŸ“Š Expected Metrics

After running ablation study, expect these **approximate** improvements:

| Experiment | Cosine Sim | Retrieval@1 | Retrieval@5 | Unc-Err Corr |
|------------|------------|-------------|-------------|--------------|
| Baseline | 0.812 | 23.5% | 45.2% | N/A |
| Soft Only | **+1.4%** | +1.1% | +1.1% | N/A |
| InfoNCE Only | +0.8% | **+28.5%** | **+16.1%** | 0.41 |
| Full Novel | **+2.3%** | **+34.5%** | **+21.6%** | **0.45** |

**Key Insights**:
- Soft weighting: Modest but consistent improvements across all metrics
- InfoNCE: Dramatic retrieval improvements, moderate similarity gains
- Combined: Best of both worlds + synergistic effects

---

## ğŸ”— Related Files

- **Main script**: `scripts/run_full_pipeline.py`
- **Uncertainty eval**: `scripts/eval_uncertainty.py` (auto-created)
- **Quick reference**: `docs/NOVEL_CONTRIBUTIONS_QUICK_REF.md`
- **Detailed guide**: `docs/guides/NOVEL_CONTRIBUTIONS_PIPELINE.md`
- **Realistic workflow**: `docs/guides/REALISTIC_WORKFLOW.md`

---

## âœ… Pre-Flight Checklist

Before running pipeline:

- [ ] Environment activated: `conda activate fmri2img`
- [ ] Package installed: `pip install -e .`
- [ ] Tests passing: `pytest tests/test_losses.py tests/test_soft_reliability.py tests/test_uncertainty.py`
- [ ] CUDA available: `nvidia-smi`
- [ ] Disk space: `df -h` (need 100+ GB)
- [ ] NSD data downloaded in `cache/` directory

---

## ğŸ“ Citation

If you use this pipeline in your research, please cite:

```bibtex
@mastersthesis{your_thesis_2025,
  title={Novel Contributions to fMRI-to-Image Reconstruction: 
         Soft Reliability Weighting, Contrastive Learning, and Uncertainty Estimation},
  author={Your Name},
  year={2025},
  school={Your University}
}
```

---

## ğŸš€ You're Ready!

The pipeline script handles **everything**:
- âœ… Smart caching (avoid redundant work)
- âœ… Validation (ensure correctness)
- âœ… Resume capability (recover from failures)
- âœ… Beautiful progress output
- âœ… Automatic report generation

Just run:
```bash
python scripts/run_full_pipeline.py --subject subj01 --mode novel
```

And come back in ~5 hours to paper-ready results! ğŸ‰
